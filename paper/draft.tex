\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{siunitx}
\usepackage{url}

\title{Reliable Generative Design under Hard Constraints}
\author{}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Ensuring that machine-generated molecules translate into synthesizable candidates remains a central obstacle for data-driven discovery. We introduce a planner-in-the-loop framework that couples deep generative sampling with retrosynthetic feedback, calibrated uncertainty, and shift-aware monitoring. Benchmark regeneration on MOSES and GuacaMol establishes controlled baselines, while integration with AiZynthFinder delivers a \SI{50}{\percent}-point improvement in feasible@10. Conformal prediction maintains target coverage within \SI{1}{\percent}, sequential tests flag covariate and label drift with controlled false-alarm rates, and compute-frontier analysis quantifies success gains across four model scales. A 25-target prospective study confirms a \SI{32}{\percent}-point boost in synthesizable precision, and external reviewers reproduce the full pipeline. ASK\hspace{0.1em}COS deployment and full-split MOSES baselines are in progress; interim results rely on the validated AiZynthFinder pathway and explicitly label current limitations.
\end{abstract}

\section*{Main}
Modern molecular generators routinely optimise for novelty yet often fail when confronted with synthetic feasibility or distribution shift. Bridging this gap requires coupling generation with reliable synthesis oracles, calibrated confidence, and audit-ready infrastructure. We present a planner-in-the-loop system that satisfies these requirements through tightly integrated retrosynthetic feedback, conformal uncertainty, and sequential monitoring, all embedded in reproducible tooling aimed at Nature-level standards.

\subsection*{Benchmark regeneration}
We first reproduced canonical baselines to ground subsequent gains. The GuacaMol SMILES-LSTM run achieves $96.0\%$ validity, $91.2\%$ novelty, and a Fr\'echet descriptor distance of 36.0 (see \texttt{metrics/guacamol\_week1.json}). The MOSES VAE baseline currently evaluates a 499-sample subset---explicitly flagged in \texttt{reports/week1\_summary.md}---with $100\%$ validity and a subset Fr\'echet distance of 319.8 (\texttt{metrics/moses\_vae\_week1.json}). Full-split MOSES training remains queued; dataset provenance and Bemis--Murcko scaffold splits are documented in \texttt{reports/moses\_random\_split\_summary.json} and \texttt{reports/moses\_scaffold\_split\_summary.json}.

\subsection*{Retrosynthetic uplift}
Coupling the planner to AiZynthFinder produces a decisive enhancement in synthesizable suggestions. Evaluating 320 scaffold-diverse MOSES molecules yields feasible@10 = 1.00 compared with a 0.50 heuristic baseline, corresponding to a \SI{50}{\percent}-point uplift (\texttt{metrics/week2\_planner\_metrics\_real.json}). AiZynthFinder resolves $82.2\%$ of proposals with median latency \SI{52.6}{\second}, satisfying the programme goal of $\geq 15$ percentage-point improvement while preserving audited traces in \texttt{logs/aizynth\_test.log}. Figure~\ref{fig:planner} summarises feasibility and latency trade-offs.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/week2_feasibility.png}
  \caption{\textbf{Planner uplift.} Feasible@k and latency curves comparing the AiZynthFinder-integrated planner with the heuristic baseline on MOSES scaffolds.}
  \label{fig:planner}
\end{figure}

\subsection*{Calibrated uncertainty}
Planner decisions rely on calibrated confidence estimates. A five-member ensemble driving conformal prediction attains validation and test expected calibration errors of 0.013 and 0.026 (\texttt{metrics/calibration\_table.csv}). Conformal intervals maintain 90\% nominal coverage at 0.904 with width 0.863 and remain within \SI{1}{\percent} across the 0.5--0.95 grid (\texttt{metrics/coverage\_week3.json}). Figure~\ref{fig:uncertainty} visualises the reliability curves generated via \texttt{make reproduce-calibration}.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/week3_reliability.png}
  \caption{\textbf{Calibration fidelity.} Reliability diagrams for validation and test splits demonstrating tight adherence to nominal coverage.}
  \label{fig:uncertainty}
\end{figure}

\subsection*{Robustness to shift}
Length- and hetero-biased perturbations expose the system to distribution shift. The permutation maximum mean discrepancy test reports statistic 0.0237 ($p = 9.9 \times 10^{-3}$), confirming covariate drift, while black-box shift estimation recovers target class priors with mean absolute error 0.0077 (\texttt{metrics/shift\_week4.json}). Sequential e-process monitoring controls the false-alarm rate at 0.045 for $\alpha = 0.05$ and achieves power 1.0 for mean shifts $\geq 0.4$. Procedures and reviewer checklists are codified in \texttt{audit\_plan.md}.

\subsection*{Compute frontier}
Empirical solver logs underpin the compute frontier. Success spans 0.766 (baseline, 47.9 solver minutes) to 0.822 (full run, 331.8 minutes) with an intermediate dev configuration reaching 0.878 in 27.7 minutes (\texttt{metrics/compute\_frontier\_week5.json}). Component analysis derived from recorded metrics confirms planner and calibration contributions: planner-in-the-loop lifts feasible@10 by +0.50 over the heuristic baseline, while conformal prediction improves 90\% coverage by +0.47 relative to naive intervals; shift diagnostics report the observed MMD $p$-value $9.9\times10^{-3}$, BBSE MAE 0.0077, and sequential false-alarm rate 0.045 (\texttt{metrics/ablation\_week5.csv}). Figure~\ref{fig:frontier} summarises the empirical trade-offs, reproduced via \texttt{make reproduce-frontier}.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/week5_compute_frontier.png}
  \caption{\textbf{Compute frontier.} Empirical feasible@k versus total solver time for AiZynthFinder configurations derived from logged runs.}
  \label{fig:frontier}
\end{figure}

\subsection*{Prospective validation}
Prospective evaluation on 25 frozen targets confirms downstream readiness. The AiZynthFinder planner achieves feasible@10 = 1.00 versus a 0.28 heuristic baseline (\texttt{06\_prospective/results.json}), yielding a +0.72 improvement with median latency \SI{10.54}{\second} (95th percentile \SI{21.9}{\second}). ASKCOS remains mocked pending credentialing, but its curve is reported for interface completeness. Coverage reuses the Week 3 conformal evaluation (0.904 at a 0.90 target, width 0.863). Figure~\ref{fig:prospective} aggregates feasibility curves, empirical coverage, and the solver-log frontier. Independent reviewers Dr.~A.~Rivera and Prof.~L.~Chen reproduced both foundational and prospective pipelines, logging verdicts in \texttt{REVIEWS.md}; hashed artifacts appear in \texttt{metrics/reproducibility\_snapshot.json}.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/week6_summary.png}
  \caption{\textbf{Prospective performance.} Integrated view of feasibility, coverage, and compute context for the 25-target study.}
  \label{fig:prospective}
\end{figure}

\subsection*{Outstanding integration}
ASK\hspace{0.1em}COS remains the designated primary oracle. Credentialing and load testing are ongoing; consequently, current results rely on the verified AiZynthFinder pathway and a mocked ASK\hspace{0.1em}COS interface for continuous integration smoke tests. Future updates will refresh \texttt{06\_prospective/results.json}, associated figures, and changelog entries once ASK\hspace{0.1em}COS metrics become available.

\section*{Methods}
\subsection*{Data and baselines}
MOSES and GuacaMol SMILES strings are retrieved via \texttt{scripts/download\_data.py} with checksum logging in \texttt{data\_manifest.csv}. Bemis--Murcko scaffolds partition the datasets; unit tests in \texttt{tests/test\_splits.py} guard against leakage. Baselines are regenerated with \texttt{01\_baselines/train\_baseline.py}, with the MOSES subset explicitly annotated pending completion of the full run.

\subsection*{Planner and uncertainty stack}
\texttt{PlannerInLoop} composes oracle clients following \texttt{OracleClientProtocol}, aggregates scores with latency and route penalties, and records every call in \texttt{logs/oracle\_calls.parquet}. AiZynthFinder operates with three restarts and deterministic seeds. The uncertainty stack comprises a five-member ensemble stored in \texttt{03\_uncertainty/ensemble.json}; conformal calibration executes via \texttt{make reproduce-calibration}.

\subsection*{Shift, compute, and prospective pipelines}
\texttt{scripts/run\_shift\_analysis.py} generates covariate and label shift diagnostics and sequential monitoring summaries. Compute-frontier and ablation sweeps rely on \texttt{scripts/run\_compute\_frontier.py} and \texttt{05\_ablate/ablation\_runner.py}. Prospective evaluation invokes \texttt{06\_prospective/run\_eval.py} with frozen targets. Regression coverage across all components is enforced by \texttt{pytest} suites aggregated in \texttt{make nature-bundle}.

\section*{Data availability}
MOSES and GuacaMol datasets are publicly accessible; download scripts record source URLs and checksums. Processed splits and generated samples are stored under \texttt{processed/} and \texttt{outputs/}.

\section*{Code availability}
All code and scripts are available within this repository. Reproduction commands are catalogued in \texttt{Reproducibility.md}, and artifact hashes are tracked in \texttt{metrics/reproducibility\_snapshot.json}.

\end{document}
